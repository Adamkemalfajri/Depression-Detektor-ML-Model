{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6375fb9c32d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fer2013.csv')\n",
    "#check data shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview first 5 row of data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check usage values\n",
    "#80% training, 10% validation and 10% test\n",
    "data.Usage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check target labels\n",
    "emotion_map = {0: 'Angry', 1: 'Digust', 2: 'Fear', 3: 'Happy', 4: 'depression', 5: 'Surprise', 6: 'Neutral'}\n",
    "emotion_counts = data['emotion'].value_counts(sort=False).reset_index()\n",
    "emotion_counts.columns = ['emotion', 'number']\n",
    "emotion_counts['emotion'] = emotion_counts['emotion'].map(emotion_map)\n",
    "emotion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar graph of the class distributions\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(emotion_counts.emotion, emotion_counts.number)\n",
    "plt.title('Class distribution')\n",
    "plt.ylabel('Number', fontsize=12)\n",
    "plt.xlabel('Emotions', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at some images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row2image(row):\n",
    "    pixels, emotion = row['pixels'], emotion_map[row['emotion']]\n",
    "    img = np.array(pixels.split())\n",
    "    img = img.reshape(48,48)\n",
    "    image = np.zeros((48,48,3))\n",
    "    image[:,:,0] = img\n",
    "    image[:,:,1] = img\n",
    "    image[:,:,2] = img\n",
    "    return np.array([image.astype(np.uint8), emotion])\n",
    "\n",
    "plt.figure(0, figsize=(16,10))\n",
    "for i in range(1,8):\n",
    "    face = data[data['emotion'] == i-1].iloc[0]\n",
    "    img = row2image(face)\n",
    "    plt.subplot(2,4,i)\n",
    "    plt.imshow(img[0])\n",
    "    plt.title(img[1])\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------\n",
    "## Pre-processing data\n",
    "#### Summary:\n",
    "1. Splitting dataset into 3 parts: train, validation, test\n",
    "1. Convert strings to lists of integers\n",
    "1. Reshape to 48x48 and normalise grayscale image with 255.0\n",
    "1. Perform one-hot encoding label, e.g. class 3 to [0,0,0,1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training, validation and test set\n",
    "data_train = data[data['Usage']=='Training'].copy()\n",
    "data_val   = data[data['Usage']=='PublicTest'].copy()\n",
    "data_test  = data[data['Usage']=='PrivateTest'].copy()\n",
    "print(\"train shape: {}, \\nvalidation shape: {}, \\ntest shape: {}\".format(data_train.shape, data_val.shape, data_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot class distribution of train, val and test\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def setup_axe(axe,df,title):\n",
    "    df['emotion'].value_counts(sort=False).plot(ax=axe, kind='bar', rot=0)\n",
    "    axe.set_xticklabels(emotion_labels)\n",
    "    axe.set_xlabel(\"Emotions\")\n",
    "    axe.set_ylabel(\"Number\")\n",
    "    axe.set_title(title)\n",
    "    \n",
    "    # set individual bar lables using above list\n",
    "    for i in axe.patches:\n",
    "        # get_x pulls left or right; get_height pushes up or down\n",
    "        axe.text(i.get_x()-.05, i.get_height()+120, \\\n",
    "                str(round((i.get_height()), 2)), fontsize=14, color='dimgrey',\n",
    "                    rotation=0)\n",
    "\n",
    "   \n",
    "fig, axes = plt.subplots(1,3, figsize=(20,8), sharey=True)\n",
    "setup_axe(axes[0],data_train,'train')\n",
    "setup_axe(axes[1],data_val,'validation')\n",
    "setup_axe(axes[2],data_test,'test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the later two subplots share the same y-axis with the first subplot. \n",
    "\n",
    "The size of **train**, **validation**, **test** are **80%**, **10%** and **10%**, respectively. \n",
    "\n",
    "The exact number of each class of these datasets are written on top of their x-axis bar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize parameters\n",
    "num_classes = 7 \n",
    "width, height = 48, 48\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "num_features = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRNO stands for Convert, Reshape, Normalize, One-hot encoding\n",
    "# (i) convert strings to lists of integers\n",
    "# (ii) reshape and normalise grayscale image with 255.0\n",
    "# (iii) one-hot encoding label, e.g. class 3 to [0,0,0,1,0,0,0]\n",
    "\n",
    "def CRNO(df, dataName):\n",
    "    df['pixels'] = df['pixels'].apply(lambda pixel_sequence: [int(pixel) for pixel in pixel_sequence.split()])\n",
    "    data_X = np.array(df['pixels'].tolist(), dtype='float32').reshape(-1,width, height,1)/255.0   \n",
    "    data_Y = to_categorical(df['emotion'], num_classes)  \n",
    "    print(dataName, \"_X shape: {}, \", dataName, \"_Y shape: {}\".format(data_X.shape, data_Y.shape))\n",
    "    return data_X, data_Y\n",
    "\n",
    "    \n",
    "train_X, train_Y = CRNO(data_train, \"train\") #training data\n",
    "val_X, val_Y     = CRNO(data_val, \"val\") #validation data\n",
    "test_X, test_Y   = CRNO(data_test, \"test\") #test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------\n",
    "## Building CNN Model\n",
    "\n",
    "### CNN Architecture:  \n",
    "* Conv -> BN -> Activation -> Conv -> BN -> Activation -> MaxPooling \n",
    "* Conv -> BN -> Activation -> Conv -> BN -> Activation -> MaxPooling \n",
    "* Conv -> BN -> Activation -> Conv -> BN -> Activation -> MaxPooling \n",
    "* Flatten\n",
    "* Dense -> BN -> Activation\n",
    "* Dense -> BN -> Activation\n",
    "* Dense -> BN -> Activation\n",
    "* Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#module 1\n",
    "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), input_shape=(width, height, 1), data_format='channels_last'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "#module 2\n",
    "model.add(Conv2D(2*num_features, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(2*num_features, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "#module 3\n",
    "model.add(Conv2D(num_features, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(num_features, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "#flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "#dense 1\n",
    "model.add(Dense(2*2*2*num_features))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#dense 2\n",
    "model.add(Dense(2*2*num_features))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#dense 3\n",
    "model.add(Dense(2*num_features))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience = 10, mode = 'min', restore_best_weights=True)\n",
    "\n",
    "history = model.fit_generator(data_generator.flow(train_X, train_Y, batch_size),\n",
    "                                steps_per_epoch=len(train_X) / batch_size,\n",
    "                                epochs=num_epochs,\n",
    "                                verbose=2, \n",
    "                                callbacks = [es],\n",
    "                                validation_data=(val_X, val_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(18, 6))\n",
    "# Plot training & validation accuracy values\n",
    "axes[0].plot(history.history['acc'])\n",
    "axes[0].plot(history.history['val_acc'])\n",
    "axes[0].set_title('Model accuracy')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "axes[1].plot(history.history['loss'])\n",
    "axes[1].plot(history.history['val_loss'])\n",
    "axes[1].set_title('Model loss')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true = np.argmax(test_Y, axis=1)\n",
    "test_pred = np.argmax(model.predict(test_X), axis=1)\n",
    "print(\"CNN Model Accuracy on test set: {:.4f}\".format(accuracy_score(test_true, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Analysis using Confusion Matrix\n",
    "\n",
    "Confusion Matrix is applied and plotted to find out which emotion usually get confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(test_true, test_pred, classes=emotion_labels, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------\n",
    "\n",
    "**Future Work:**\n",
    "\n",
    "    (i) To further fine tuning model using grid_search, specifically:\n",
    "        a. Different optimizer such as Adam, RMSprop, Adagrad.\n",
    "        b. experimenting dropout with batch-normalization.\n",
    "        c. experimenting different dropout rates. \n",
    "\n",
    "    (ii) To collect more data and train the model with balance dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------\n",
    "### Extra:\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "#### (i) Apply Batch-Normalization before or after activation function?\n",
    "*Regarding this issue, it is still occasionally a topic of debate. However, in this project, we applied BN before ReLu as we are trying to follow the original BatchNorm paper. The following is the exact text from the paper...*\n",
    "    \n",
    "> We add the BN transform immediately before the nonlinearity, by normalizing x = Wu+ b. \n",
    "We could have also normalized the layer inputs u, but since u is likely the output of another nonlinearity, \n",
    "the shape of its distribution is likely to change during training, and constraining its first and second \n",
    "moments would not eliminate the covariate shift. In contrast, Wu + b is more likely to have a symmetric, \n",
    "non-sparse distribution, that is “more Gaussian” (Hyv¨arinen & Oja, 2000); normalizing it is likely to \n",
    "produce activations with a stable distribution.\n",
    "\n",
    "\n",
    "#### (ii) Is it better to use DropOut together with Batch-Normalization?\n",
    "*Again, it is another common debate topic in DL community. In this project, we totally eliminate the use of dropout and focusing batch-normalization technique only. This is because, Batch normalization already offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.* \n",
    "\n",
    "> Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout\n",
    "\n",
    "More details -> [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It was a fun project, Happy Kaggling :) *\n",
    "-----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
